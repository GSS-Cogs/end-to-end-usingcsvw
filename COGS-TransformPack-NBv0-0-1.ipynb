{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COGS Transform Pack:\n",
    "**This basic Jupyter Notebook is intended to aid the COGS transformation process(es) of human readable to machine readable data. This Notebook will be used to house the Python definitions and functions that can be imported into the main transform script. Longer term, if deemed useful these functions could form the foundation of a developed Python library.**\n",
    "### Section: Define and Load into Memory Notebook Python Libraries / Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge cchardet\n",
    "#!pip install chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: Future update to include Python Doc Strings etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc Strings / About etc:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: csvwlib in /anaconda3/lib/python3.7/site-packages (0.2.3)\n",
      "Requirement already satisfied: uritemplate==3.0.0 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil==2.6.1 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (2.6.1)\n",
      "Requirement already satisfied: requests==2.18.4 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (2.18.4)\n",
      "Requirement already satisfied: language-tags==0.4.3 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (0.4.3)\n",
      "Requirement already satisfied: rdflib==4.2.2 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (4.2.2)\n",
      "Requirement already satisfied: rdflib-jsonld==0.4.0 in /anaconda3/lib/python3.7/site-packages (from csvwlib) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil==2.6.1->csvwlib) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests==2.18.4->csvwlib) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests==2.18.4->csvwlib) (1.22)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests==2.18.4->csvwlib) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests==2.18.4->csvwlib) (3.0.4)\n",
      "Requirement already satisfied: pyparsing in /anaconda3/lib/python3.7/site-packages (from rdflib==4.2.2->csvwlib) (2.4.0)\n",
      "Requirement already satisfied: isodate in /anaconda3/lib/python3.7/site-packages (from rdflib==4.2.2->csvwlib) (0.6.0)\n",
      "Requirement already satisfied: slug in /anaconda3/lib/python3.7/site-packages (2.0)\n",
      "Requirement already satisfied: goodtables in /anaconda3/lib/python3.7/site-packages (2.4.0)\n",
      "Requirement already satisfied: tableschema>=1.10 in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.10.0)\n",
      "Requirement already satisfied: datapackage>=1.10 in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.10.0)\n",
      "Requirement already satisfied: click>=6.6 in /anaconda3/lib/python3.7/site-packages (from goodtables) (7.0)\n",
      "Requirement already satisfied: simpleeval>=0.9 in /anaconda3/lib/python3.7/site-packages (from goodtables) (0.9.8)\n",
      "Requirement already satisfied: six>=1.9 in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.12.0)\n",
      "Requirement already satisfied: click-default-group in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.2.2)\n",
      "Requirement already satisfied: requests>=2.10 in /anaconda3/lib/python3.7/site-packages (from goodtables) (2.18.4)\n",
      "Requirement already satisfied: statistics>=1.0 in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.0.3.5)\n",
      "Requirement already satisfied: tabulator>=1.29 in /anaconda3/lib/python3.7/site-packages (from goodtables) (1.29.0)\n",
      "Requirement already satisfied: jsonschema>=2.5 in /anaconda3/lib/python3.7/site-packages (from tableschema>=1.10->goodtables) (3.0.1)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /anaconda3/lib/python3.7/site-packages (from tableschema>=1.10->goodtables) (0.6.0)\n",
      "Requirement already satisfied: rfc3986>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tableschema>=1.10->goodtables) (1.3.2)\n",
      "Requirement already satisfied: unicodecsv>=0.14 in /anaconda3/lib/python3.7/site-packages (from tableschema>=1.10->goodtables) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /anaconda3/lib/python3.7/site-packages (from tableschema>=1.10->goodtables) (2.6.1)\n",
      "Requirement already satisfied: cchardet>=1.0 in /anaconda3/lib/python3.7/site-packages (from datapackage>=1.10->goodtables) (2.1.4)\n",
      "Requirement already satisfied: jsonpointer>=1.10 in /anaconda3/lib/python3.7/site-packages (from datapackage>=1.10->goodtables) (2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10->goodtables) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10->goodtables) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10->goodtables) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10->goodtables) (2.6)\n",
      "Requirement already satisfied: docutils>=0.3 in /anaconda3/lib/python3.7/site-packages (from statistics>=1.0->goodtables) (0.14)\n",
      "Requirement already satisfied: linear-tsv>=1.0 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (1.1.0)\n",
      "Requirement already satisfied: ijson>=2.5 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (2.5.1)\n",
      "Requirement already satisfied: jsonlines>=1.1 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (1.2.0)\n",
      "Requirement already satisfied: boto3>=1.9 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (1.10.14)\n",
      "Requirement already satisfied: sqlalchemy>=0.9.6 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (1.3.5)\n",
      "Requirement already satisfied: xlrd>=1.0 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (1.2.0)\n",
      "Requirement already satisfied: openpyxl>=2.6 in /anaconda3/lib/python3.7/site-packages (from tabulator>=1.29->goodtables) (2.6.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from jsonschema>=2.5->tableschema>=1.10->goodtables) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /anaconda3/lib/python3.7/site-packages (from jsonschema>=2.5->tableschema>=1.10->goodtables) (0.14.11)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from jsonschema>=2.5->tableschema>=1.10->goodtables) (41.0.1)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from boto3>=1.9->tabulator>=1.29->goodtables) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.7/site-packages (from boto3>=1.9->tabulator>=1.29->goodtables) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /anaconda3/lib/python3.7/site-packages (from boto3>=1.9->tabulator>=1.29->goodtables) (1.13.14)\n",
      "Requirement already satisfied: et-xmlfile in /anaconda3/lib/python3.7/site-packages (from openpyxl>=2.6->tabulator>=1.29->goodtables) (1.0.1)\n",
      "Requirement already satisfied: jdcal in /anaconda3/lib/python3.7/site-packages (from openpyxl>=2.6->tabulator>=1.29->goodtables) (1.4.1)\n",
      "Requirement already satisfied: PyGithub in /anaconda3/lib/python3.7/site-packages (1.46)\n",
      "Requirement already satisfied: requests>=2.14.0 in /anaconda3/lib/python3.7/site-packages (from PyGithub) (2.18.4)\n",
      "Requirement already satisfied: pyjwt in /anaconda3/lib/python3.7/site-packages (from PyGithub) (1.7.1)\n",
      "Requirement already satisfied: deprecated in /anaconda3/lib/python3.7/site-packages (from PyGithub) (1.2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.14.0->PyGithub) (2.6)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /anaconda3/lib/python3.7/site-packages (from deprecated->PyGithub) (1.11.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install csvwlib\n",
    "from csvwlib import CSVWConverter\n",
    "\n",
    "!pip install slug\n",
    "import slug\n",
    "\n",
    "!pip install goodtables\n",
    "from goodtables import validate\n",
    "from goodtables import Inspector\n",
    "inspector = Inspector()\n",
    "\n",
    "!pip install PyGithub\n",
    "from github import Github\n",
    "\n",
    "#!pip install PyDrive\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for markdown Notebook outputs:\n",
    "# Paramaters: string (string) - string to display, colour (String) - colour of output:\n",
    "# Dependencies:\n",
    "def printmd(string, colour=None):\n",
    "    colourstr = \"<span style='color:{}'>{}</span>\".format(colour, string)\n",
    "    display(Markdown(colourstr))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that determines Python Execution Environment:\n",
    "# Dependencies: printmd\n",
    "def boo_pythonNB_environment():\n",
    "    if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        boo_pythonNB_environment = True\n",
    "    else:\n",
    "        boo_pythonNB_environment = False\n",
    "\n",
    "    printmd('***Execution Environment: ' + get_ipython().__class__.__name__ + '; setting boo_pythonNB_environment to: ' \\\n",
    "          + str(boo_pythonNB_environment) + '.***', colour='Grey')\n",
    "    \n",
    "    return boo_pythonNB_environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple display of DataFrame Collections.\n",
    "# Paramaters: dataframe_collection, row_display_limit blah blah to update...\n",
    "# Dependencies: printmd\n",
    "def display_DF_collection(dataframe_collection, str_title = None, row_display_limit = 10):\n",
    "    if str_title != None:\n",
    "        printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "        printmd('**' + str_title + '**' + ' ', colour = 'Magenta')\n",
    "        printmd(\"=\"*115, colour='Grey')       \n",
    "    for key in dataframe_collection.keys():\n",
    "        printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "        printmd('**' + key + '**' + ' *: First ' + str(row_display_limit) + ' records displayed of*' + ' ' + str(dataframe_collection[key].shape[0]) + ' records.', colour='Blue')\n",
    "        printmd(\"=\"*115, colour='Grey')\n",
    "        #print(dataframe_collection[key]) #Print like this for Logs...  \n",
    "        display(dataframe_collection[key].head(row_display_limit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple display of DataFrame Collections.\n",
    "# Paramaters: dataframe_collection, row_display_limit blah blah to update...\n",
    "# Dependencies: printmd, slug\n",
    "def display_DF_collection_csv_report(dataframe_collection):\n",
    "    for key in dataframe_collection.keys():\n",
    "        csv_path_file = str(slug.slug(key)) + '.csv'\n",
    "        dataframe_collection[key].to_csv(csv_path_file, index = None, header=True)\n",
    "        report = validate(csv_path_file)\n",
    "        printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "        printmd('***CSV extract for: ' + csv_path_file + ' has been examined. Validation of the file is:***' + ' ' + str(report['valid']) + '.', colour='Purple')\n",
    "        printmd(\"=\"*115, colour='Grey')\n",
    "        if report['valid'] == False:\n",
    "            closer_file_inspection = inspector.inspect(csv_path_file)\n",
    "            pprint(closer_file_inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all data elements (columns) from data outputs (tidy data):\n",
    "# Paramaters: dataframe_collection blah blah to update...\n",
    "# Dependencies: printmd\n",
    "def get_component_elements(dataframe_collection):\n",
    "    codelist_cols = []\n",
    "    i = 0\n",
    "    for key in dataframe_collection.keys():\n",
    "        codelist_cols.append((list(dataframe_collection[key].columns)))\n",
    "        printmd('**Extracted columns from dataset ' + key + \":**\", colour='Green')\n",
    "        printmd(codelist_cols[i], colour='Grey')  \n",
    "        i += 1\n",
    "    return codelist_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a collection of DataFrames - default all entries to dimensions intentionally\n",
    "# - they need to be assigned manually.\n",
    "# Dependencies:\n",
    "def get_transform_component_schema(dataframe_collection, codelist_cols):\n",
    "    dataframe_elements_collection = {}\n",
    "    i = 0\n",
    "    for key in dataframe_collection.keys():\n",
    "        for x in range(len(codelist_cols[i])):\n",
    "            df_components = pd.DataFrame(codelist_cols[i],columns=[key])\n",
    "        df_components['Entry Type']='Dimension'\n",
    "        dataframe_elements_collection[key] = df_components\n",
    "        i += 1\n",
    "    return dataframe_elements_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign collection of DataFrame components to their type (Measure or Observation):\n",
    "# Code is in-efficient due to multiple passes of the collections per call.\n",
    "# Original code was contained in a single loop but this component has easier usability...\n",
    "# future code revision could improve performance by str_components and str_type_components being passed as arrays.\n",
    "# Dependencies:\n",
    "def assign_transform_component_type(dataframe_elements_collection, str_component, str_type_component):\n",
    "    switch = 'Proceed'\n",
    "    if (str_type_component == 'Measure') or (str_type_component == 'M') or (str_type_component == 'm'):\n",
    "        type_component = 'Measure'\n",
    "    elif (str_type_component == 'Observation') or (str_type_component == 'O') or (str_type_component == 'o'):\n",
    "        type_component = 'Observation'\n",
    "    else:\n",
    "        switch = 'Error'\n",
    "    if switch != 'Error':\n",
    "        for key in dataframe_elements_collection.keys():\n",
    "            dataframe_elements_collection[key].loc[dataframe_elements_collection[key][key] == str_component, ['Entry Type']] = type_component\n",
    "    return dataframe_elements_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate codelists from a collection of DataFrames - using assigned dimenison element type.\n",
    "# Dependencies:\n",
    "def get_codelists_from_dimensions(dataframe_collection, dataframe_elements_collection):\n",
    "    dataframe_codelists_collection = {}\n",
    "    for key in dataframe_elements_collection.keys():\n",
    "        df_result = pd.DataFrame(columns = ['DropMe'])\n",
    "\n",
    "        df_temp = dataframe_elements_collection[key].loc[dataframe_elements_collection[key]['Entry Type'] == 'Dimension']\n",
    "        #display(df_temp)\n",
    "        codelist_cols = []\n",
    "        for rows in df_temp.itertuples():\n",
    "            codelist_cols_temp = rows[1]\n",
    "            codelist_cols.append(codelist_cols_temp)\n",
    "\n",
    "\n",
    "        for col in dataframe_collection[key]:\n",
    "            if col in codelist_cols:\n",
    "                my_codelist_lst = dataframe_collection[key][col].unique()\n",
    "                df_codelist = pd.DataFrame(my_codelist_lst, columns = [col])\n",
    "                df_result = pd.concat([df_result, df_codelist], axis = 1, ignore_index=False, sort=False)\n",
    "        df_result = df_result.drop('DropMe', 1)\n",
    "        dataframe_codelists_collection[key] = df_result\n",
    "    return dataframe_codelists_collection\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join our Slugized transformed data with our REF data - using DataFrames only - no CSV functionality:\n",
    "# Dependencies: \n",
    "def __compute_REFData_with_Transform__(input_df, source_ref_columns_df: pd.DataFrame, source_ref_components_df: pd.DataFrame):\n",
    "    \n",
    "    REFdata_intermediate = pd.merge(input_df, source_ref_columns_df, left_on='REFColumnsCSV Link', right_on='title', how='inner')\n",
    "    REFdata_linked_successful = pd.merge(REFdata_intermediate, source_ref_components_df, left_on='title', right_on='Label', how='left')\n",
    "    \n",
    "    REFdata_linked_UNsuccessful = pd.merge(input_df, source_ref_columns_df, left_on='REFColumnsCSV Link', right_on='title', how='outer', indicator=True).query('_merge==\"left_only\"')    \n",
    "\n",
    "    return REFdata_linked_successful, REFdata_linked_UNsuccessful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares a copy of the data elements to modify. We need to keep the original data elements for later processing.\n",
    "# Remember: we're passing collections of DataFrames (which hold the data elements).\n",
    "# Purpose - useability of Notebooks - reduces loops and complexity.\n",
    "# The REFColumnsCSV_Link was originally in place as I presumed incorrectly we were matching with Slugized Elements.\n",
    "# Dependencies:\n",
    "def get_mapped_elements(dataframe_elements_collection):\n",
    "    dataframe_mapped_elements_collection = {}\n",
    "    # Prepare mapping in memory component:\n",
    "    for key in dataframe_elements_collection.keys(): \n",
    "        dataframe_mapped_elements_collection[key] = dataframe_elements_collection[key].copy()\n",
    "        for cols in dataframe_mapped_elements_collection[key][key]:\n",
    "            idx_temp = dataframe_mapped_elements_collection[key].index[dataframe_mapped_elements_collection[key][key] == cols]\n",
    "            dataframe_mapped_elements_collection[key].loc[idx_temp, 'REFColumnsCSV Link'] = cols #slug.slug(cols) Not matched on Slugilzed as first believed!\n",
    "\n",
    "    return dataframe_mapped_elements_collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated mapping of transformed components with the master reference data:\n",
    "# Dependencies: get_mapped_elements, align_REFdata_with_Transform\n",
    "def map_REFData_with_Transform(dataframe_elements_collection, df_ref_repo_columns: pd.DataFrame, df_ref_repo_components: pd.DataFrame, execute_flag = 'AutoMap'):\n",
    "    dataframe_mapped_elements_collection = {}\n",
    "    dataframe_mapped_elements_collection_errors = {}\n",
    "\n",
    "    if execute_flag == 'AutoMap':\n",
    "        dataframe_mapped_elements_collection = get_mapped_elements(dataframe_elements_collection)\n",
    "    \n",
    "    if execute_flag != 'AutoMap':\n",
    "        dataframe_mapped_elements_collection = dataframe_elements_collection\n",
    "    \n",
    "    for key in dataframe_mapped_elements_collection.keys():\n",
    "        df_collection_point, df_collection_point_errors = __compute_REFData_with_Transform__(dataframe_mapped_elements_collection[key], df_ref_repo_columns, df_ref_repo_components)\n",
    "        dataframe_mapped_elements_collection[key] = df_collection_point\n",
    "        dataframe_mapped_elements_collection_errors[key] = df_collection_point_errors  \n",
    "    \n",
    "    return dataframe_mapped_elements_collection, dataframe_mapped_elements_collection_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually map the transform component with the user defined reference data element:\n",
    "# Dependencies: slug\n",
    "def assign_reference_data_mapping(dataframe_collection, str_component, str_component_new_map, slug_it = False):\n",
    "    if slug_it == True:\n",
    "        str_component_new_map = slug.slug(str_component_new_map)\n",
    "    for key in dataframe_collection.keys():\n",
    "        dataframe_collection[key][key].loc[dataframe_collection[key][key] == str_component] = str_component_new_map\n",
    "    return dataframe_collection\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the JSON meta file table schema - WARNING! This is a seriously hacked up <deleted swear words!>. It needs refactoring ASAP:\n",
    "# Dependencies: __get_meta_json_table_schema\n",
    "def __get_meta_json_table_schema(dataframe_mapped_elements_collection, json_meta_data_collection):\n",
    "\n",
    "    for key in json_meta_data_collection.keys():\n",
    "        scary_movie_string = ''\n",
    "        for idx, val in enumerate(dataframe_mapped_elements_collection[key].itertuples()):\n",
    "            scary_movie_string = scary_movie_string + ('\\n{ \\n\"title\": ' \\\n",
    "                                + str(dataframe_mapped_elements_collection[key].title[idx]).rstrip() \\\n",
    "                                + '\",')\n",
    "            scary_movie_string = scary_movie_string + ('\\n\"name\": ' \\\n",
    "                                + str(dataframe_mapped_elements_collection[key].name[idx]).rstrip() \\\n",
    "                                + '\",')\n",
    "            if pd.notna(dataframe_mapped_elements_collection[key].Codelist[idx]):\n",
    "                scary_movie_string = scary_movie_string + ('\\n\"propertyUrl\": ' \\\n",
    "                                    + str(dataframe_mapped_elements_collection[key].Codelist[idx]).rstrip() \\\n",
    "                                    + '\",')\n",
    "            if pd.notna(dataframe_mapped_elements_collection[key].regex[idx]):\n",
    "                scary_movie_string = scary_movie_string + ('\\n\"datatype\": {\"format\": \"' \\\n",
    "                                    + str(dataframe_mapped_elements_collection[key].regex[idx]).rstrip() \\\n",
    "                                    + '\"},')\n",
    "            scary_movie_string = scary_movie_string + ('\\n\"required\": true \\n},')\n",
    "\n",
    "        json_meta_data_collection[key] = json_meta_data_collection[key] + scary_movie_string[:-1]\n",
    "        json_meta_data_collection[key] = json_meta_data_collection[key] + ('\\n]\\n}\\n}')\n",
    "\n",
    "    return json_meta_data_collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the JSON meta file - WARNING! This is a seriously hacked up <deleted swear words!>. It needs refactoring ASAP:\n",
    "# Dependencies: __get_meta_json_table_schema\n",
    "def create_meta_json(dataframe_mapped_elements_collection, dataframe_mapped_elements_collection_names, str_distribution_meta):\n",
    "    \n",
    "    json_metadata_collection = {}\n",
    "    \n",
    "    json_metadata_string_initial = ('{ \\n\"@context\": \"http://www.w3.org/ns/csvw\", ') # {\"@language\": \"en\"}') # \\n],')\n",
    "    str_distribution_meta = str_distribution_meta.replace(\"\\'\", \"\\\"\")\n",
    "    str_distribution_meta_pt2 = str_distribution_meta.split('\"')\n",
    "\n",
    "    str_parsed_meta = []\n",
    "    for i in range(len(str_distribution_meta_pt2)):\n",
    "        if i % 2 != 0:\n",
    "            str_parsed_meta.append(str_distribution_meta_pt2[i])\n",
    "\n",
    "    str_parsed_meta_links = {}\n",
    "    for i in range(len(str_parsed_meta)):\n",
    "        if (str_parsed_meta[i][:7] != 'http://') and (str_parsed_meta[i+1][:7] == 'http://'):\n",
    "            str_parsed_meta_links[str_parsed_meta[i]] = str_parsed_meta[i+1]\n",
    "\n",
    "    hacked_string_baby = ''\n",
    "    for key in str_parsed_meta_links.keys():\n",
    "        hacked_string_baby = hacked_string_baby + ('\\n\"' + str(key).rstrip() + '\": ' + '\"' + str(str_parsed_meta_links[key]).rstrip() + '\", ').rstrip()\n",
    "\n",
    "    for entry in range(len(dataframe_mapped_elements_collection_names)): \n",
    "        json_metadata_collection[dataframe_mapped_elements_collection_names[entry]] = json_metadata_string_initial + hacked_string_baby + '\\n\"url: \"' + dataframe_mapped_elements_collection_names[entry] + '\",' + '\\ntableSchema\": { \\n\"columns\": ['\n",
    "    \n",
    "    table_schema = __get_meta_json_table_schema(dataframe_mapped_elements_collection, json_metadata_collection)\n",
    "    \n",
    "    return json_metadata_collection\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RDF - I'm sure it is possible to use a local file (file://) but couldn't get it working.\n",
    "# Therefore, since this whole section of the pipeline requires a refactor I'm just using the RDF HTTP request.\n",
    "# GIT is where the CSV and META will be pushed for the RDF to be created.\n",
    "# Dependencies: \n",
    "def create_rdf(dataframe_collection):\n",
    "\n",
    "    my_pb_max = len(dataframe_collection)\n",
    "    my_pb1_calc = 0\n",
    "    \n",
    "    for key in dataframe_collection.keys():\n",
    "        \n",
    "        str_csv_for_rdf = str(slug.slug(key) + '.csv')\n",
    "        str_meta_for_rdf = str_csv_for_rdf[:-4] + '-metadata.json'\n",
    "        str_file_for_rdf = str_csv_for_rdf[:-4] + '.rdf'\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        my_pb2_calc = 0\n",
    "        \n",
    "        printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "        printmd('RDF Component Processing:', colour='Green')\n",
    "        img_prgress_bar_1 = IntProgress(min = 0, max = my_pb_max, description = 'Total:') # instantiate the bar\n",
    "        img_prgress_bar_2 = IntProgress(min = 0, max = 6, description = 'Stage:') # instantiate the bar\n",
    "        display(img_prgress_bar_1)\n",
    "        display(img_prgress_bar_2)\n",
    "        printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "\n",
    "        img_prgress_bar_1.value = my_pb1_calc # signal to increment the progress bar\n",
    "\n",
    "        \n",
    "        printmd('Identifying mandatory files: ' + str_csv_for_rdf + '--' + str_meta_for_rdf + '--' + str_file_for_rdf + '...', colour = 'Blue')\n",
    "        my_pb2_calc += 1\n",
    "        img_prgress_bar_2.value = my_pb2_calc\n",
    "\n",
    "        token=(\"aa16f221b7d3168db8053bd74d4a6fabac79de60\") # For one user who has access to one GIT directory.\n",
    "        g=Github(login_or_token=token)#,base_url=url)\n",
    "        #display(g)\n",
    "        g.get_user()\n",
    "        for repo in g.get_user().get_repos():\n",
    "            if repo.name == 'Pipeline_Processing': #print('Found Repository: ' + repo.name)\n",
    "                myRepo = repo\n",
    "                printmd('GIT Repository Identified: ' + str(myRepo) + '...', colour = 'Red')\n",
    "                my_pb2_calc += 1\n",
    "                img_prgress_bar_2.value = my_pb2_calc\n",
    "                break\n",
    "\n",
    "        printmd('Preparing files for processing: ' + str_csv_for_rdf + '...', colour = 'Blue')\n",
    "        file_csv_for_rdf = open(str_csv_for_rdf, \"r\")\n",
    "\n",
    "        printmd('Preparing files for processing: ' + str_meta_for_rdf + '...', colour = 'Blue')\n",
    "        file_meta_for_rdf = open(str_meta_for_rdf, \"r\")\n",
    "\n",
    "        printmd('Preparing files for processing: ' + str_file_for_rdf + '...', colour = 'Blue')    \n",
    "        file_rdf_package = open(str_file_for_rdf, \"w\")\n",
    "\n",
    "        my_pb2_calc += 1\n",
    "        img_prgress_bar_2.value = my_pb2_calc\n",
    "        \n",
    "        myRepo.create_file(str_csv_for_rdf, (\"content = Upload RDF-Processing for: \" + str_csv_for_rdf), file_csv_for_rdf.read()) #, branch=\"master\")\n",
    "        myRepo.create_file(str_meta_for_rdf, (\"content = Upload RDF-Processing for: \" + str_meta_for_rdf), file_meta_for_rdf.read()) #, branch=\"master\")\n",
    "\n",
    "        my_pb2_calc += 1\n",
    "        img_prgress_bar_2.value = my_pb2_calc        \n",
    "        \n",
    "        rdf_package = CSVWConverter.to_rdf(str('https://raw.githubusercontent.com/GSS-Cogs/Pipeline_Processing/master/' + str_csv_for_rdf), format='ttl')\n",
    "        file_rdf_package.write(rdf_package)\n",
    "        file_rdf_package.close()\n",
    "        printmd('RDF Generation Successful for: ' + str_file_for_rdf + '.', colour = 'Green')\n",
    "\n",
    "        my_pb2_calc += 1\n",
    "        img_prgress_bar_2.value = my_pb2_calc \n",
    "        \n",
    "        printmd('Repository Clean-up started: ' + str(myRepo) + '...', colour = 'Blue')\n",
    "        contents = repo.get_contents(str_csv_for_rdf)#, ref=\"RDF\")\n",
    "        myRepo.delete_file(contents.path, \"Clean Down\", contents.sha, branch=\"master\")\n",
    "        contents = repo.get_contents(str_meta_for_rdf)#, ref=\"RDF\")\n",
    "        myRepo.delete_file(contents.path, \"Clean Down\", contents.sha, branch=\"master\")\n",
    "\n",
    "        my_pb2_calc += 1\n",
    "        img_prgress_bar_2.value = my_pb2_calc \n",
    "        \n",
    "        my_pb1_calc += 1 # Overall Progress.\n",
    "        time.sleep(2) # 2 secs - Really?\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "    printmd('RDF Component Processing:', colour='Green')\n",
    "    img_prgress_bar_1 = IntProgress(min = 0, max = 1, description = 'Total:') # instantiate the bar\n",
    "    img_prgress_bar_2 = IntProgress(min = 0, max = 1, description = 'Stage:') # instantiate the bar\n",
    "    display(img_prgress_bar_1)\n",
    "    display(img_prgress_bar_2)\n",
    "    img_prgress_bar_1.value = 1\n",
    "    img_prgress_bar_2.value = 1\n",
    "    printmd('***Process COMPLETE!***', colour = 'Green')\n",
    "    printmd(\"\\n\" + \"=\"*115, colour='Grey')\n",
    "\n",
    "\n",
    "    return None # Refactor to report status?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
