{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COGS Transform Demonstrator:\n",
    "\n",
    "**Description:** *This basic Jupyter Notebook is intended to demonstrate the end-to-end process using a number of defined steps. These steps do not need to be coupled within a single Notebook.*\n",
    "\n",
    "***Note:*** *Sections of this transform are not yet fully developed; some developer first cut / hacks are present.*\n",
    "\n",
    "**Status:** *Prototype Version 2.2.0.*\n",
    "\n",
    "**Authour:** *Martyn Spooner.*\n",
    "\n",
    "**Create Date:** *1st February 2020 (late 2019 component protobook pre-date this).*\n",
    "\n",
    "***Revision History:***\n",
    "* *1.x.x - Various hacked pieces of independent code eg: GoodTables CSVWLib etc. - M. Spooner Late 2019.*\n",
    "* *2.0.0 - Packaging of 'hacked work pieces' into an end-to-end process - M. Spooner 1st February 2020.*\n",
    "* *2.1.0 - Re-housing of core code components to a module - M. Spooner 12th February 2020.*\n",
    "* *2.2.0 - Codelist recommendation component(s) & RDF creation refactoring - M. Spooner 26th February 2020.*\n",
    "\n",
    "### Section: Define and Load into Memory Notebook Python Libraries / Components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gssutils in /anaconda3/lib/python3.7/site-packages (0.5.12)\n",
      "Requirement already satisfied: CacheControl in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.12.5)\n",
      "Requirement already satisfied: lockfile in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.12.2)\n",
      "Requirement already satisfied: argparse in /anaconda3/lib/python3.7/site-packages (from gssutils) (1.4.0)\n",
      "Requirement already satisfied: messytables in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.15.1)\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.7/site-packages (from gssutils) (4.3.4)\n",
      "Requirement already satisfied: pyexcel in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.5.15)\n",
      "Requirement already satisfied: pyexcel-io in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.5.20)\n",
      "Requirement already satisfied: pyexcel-ods3 in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.5.3)\n",
      "Requirement already satisfied: rdflib in /anaconda3/lib/python3.7/site-packages (from gssutils) (4.2.2)\n",
      "Requirement already satisfied: pyexcel-xls in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.5.8)\n",
      "Requirement already satisfied: python-dateutil in /anaconda3/lib/python3.7/site-packages (from gssutils) (2.6.1)\n",
      "Requirement already satisfied: html2text in /anaconda3/lib/python3.7/site-packages (from gssutils) (2019.8.11)\n",
      "Requirement already satisfied: unidecode in /anaconda3/lib/python3.7/site-packages (from gssutils) (1.1.1)\n",
      "Requirement already satisfied: wheel in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.33.4)\n",
      "Requirement already satisfied: rdflib-jsonld in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.4.0)\n",
      "Requirement already satisfied: databaker in /anaconda3/lib/python3.7/site-packages (from gssutils) (2.0.0)\n",
      "Requirement already satisfied: ipython in /anaconda3/lib/python3.7/site-packages (from gssutils) (7.6.1)\n",
      "Requirement already satisfied: pandas in /anaconda3/lib/python3.7/site-packages (from gssutils) (0.24.2)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from gssutils) (2.18.4)\n",
      "Requirement already satisfied: xypath==1.1.1 in /anaconda3/lib/python3.7/site-packages (from gssutils) (1.1.1)\n",
      "Requirement already satisfied: msgpack in /anaconda3/lib/python3.7/site-packages (from CacheControl->gssutils) (0.6.1)\n",
      "Requirement already satisfied: html5lib in /anaconda3/lib/python3.7/site-packages (from messytables->gssutils) (1.0.1)\n",
      "Requirement already satisfied: python-magic>=0.4.12 in /anaconda3/lib/python3.7/site-packages (from messytables->gssutils) (0.4.15)\n",
      "Requirement already satisfied: chardet>=2.3.0 in /anaconda3/lib/python3.7/site-packages (from messytables->gssutils) (3.0.4)\n",
      "Requirement already satisfied: json-table-schema<=0.2.1,>=0.2 in /anaconda3/lib/python3.7/site-packages (from messytables->gssutils) (0.2.1)\n",
      "Requirement already satisfied: xlrd>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from messytables->gssutils) (1.2.0)\n",
      "Requirement already satisfied: texttable>=0.8.2 in /anaconda3/lib/python3.7/site-packages (from pyexcel->gssutils) (1.6.2)\n",
      "Requirement already satisfied: lml>=0.0.4 in /anaconda3/lib/python3.7/site-packages (from pyexcel->gssutils) (0.0.9)\n",
      "Requirement already satisfied: pyexcel-ezodf>=0.3.3 in /anaconda3/lib/python3.7/site-packages (from pyexcel-ods3->gssutils) (0.3.4)\n",
      "Requirement already satisfied: pyparsing in /anaconda3/lib/python3.7/site-packages (from rdflib->gssutils) (2.4.0)\n",
      "Requirement already satisfied: isodate in /anaconda3/lib/python3.7/site-packages (from rdflib->gssutils) (0.6.0)\n",
      "Requirement already satisfied: xlwt in /anaconda3/lib/python3.7/site-packages (from pyexcel-xls->gssutils) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.7/site-packages (from python-dateutil->gssutils) (1.12.0)\n",
      "Requirement already satisfied: xlutils in /anaconda3/lib/python3.7/site-packages (from databaker->gssutils) (2.0.0)\n",
      "Requirement already satisfied: pyhamcrest in /anaconda3/lib/python3.7/site-packages (from databaker->gssutils) (1.9.0)\n",
      "Requirement already satisfied: docopt in /anaconda3/lib/python3.7/site-packages (from databaker->gssutils) (0.6.2)\n",
      "Requirement already satisfied: pygments in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (2.4.2)\n",
      "Requirement already satisfied: pickleshare in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (0.7.5)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (4.7.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (4.3.2)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (4.4.0)\n",
      "Requirement already satisfied: backcall in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (0.13.3)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (2.0.9)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /anaconda3/lib/python3.7/site-packages (from ipython->gssutils) (41.0.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /anaconda3/lib/python3.7/site-packages (from pandas->gssutils) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.7/site-packages (from pandas->gssutils) (2019.1)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->gssutils) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->gssutils) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->gssutils) (1.22)\n",
      "Requirement already satisfied: webencodings in /anaconda3/lib/python3.7/site-packages (from html5lib->messytables->gssutils) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->gssutils) (0.6.0)\n",
      "Requirement already satisfied: ipython-genutils in /anaconda3/lib/python3.7/site-packages (from traitlets>=4.2->ipython->gssutils) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.3.0 in /anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython->gssutils) (0.5.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->gssutils) (0.1.7)\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "\n",
    "!pip install gssutils\n",
    "from gssutils import *\n",
    "\n",
    "# In development Transform Library to aid transforms:\n",
    "import COGSTransformPack\n",
    "\n",
    "# DataFrames!\n",
    "import pandas as pd\n",
    "\n",
    "# Useful for testing etc.\n",
    "import numpy as np\n",
    "\n",
    "# Do not display warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#%load_ext autoreload # For Development Purposes...\n",
    "#%autoreload 2 # For Development Purposes...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage Nulla: The Data Engineering Specification & Transform Notes:\n",
    "\n",
    "***The specification should be in play before any transformation coding begins unless agreed with the wider team and decision makers!***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/TransformDesign_HLE_Synthetic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HELP ME LEIGH: We need to define the specification format / schema (it is possible we can later use this for automation). The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc.The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc. The above images are obviously synthetic, this cell and others would normally contain the specification etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage I: Harvesting Human Readable Data Sources:\n",
    "* **Using GSS_Utils we can web scrape data sources (such as .XLSX spreadsheets and other formats).**\n",
    "* **Using the Scraper component of GSS-Utils we will display all the identified distributions of data at our defined landing page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amend the landing page string as required to locate your file distributions:\n",
    "\n",
    "str_http_landing_page = 'https://www.ons.gov.uk/peoplepopulationandcommunity/healthandsocialcare/' \\\n",
    "                          'healthandlifeexpectancies/datasets/healthstatelifeexpectancyallagesuk'\n",
    "\n",
    "scraper = Scraper(str_http_landing_page)\n",
    "display(scraper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage II: Identifying the Distribution to Extract and Process:\n",
    "**For our demonstration we're selecting the first distribution (.xls) for processing...**\n",
    "\n",
    "***Note:*** *our component scrapes associated meta data that we'll need later...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the list above we're selecting the first distribution: [0]. Each 'tab' will hold relevant Spreadsheet tabs\n",
    "# from the encapsulated distribution - in out case all the tabs that are within a spreadsheet...\n",
    "\n",
    "tabs = scraper.distributions[0].as_databaker()\n",
    "distribution = scraper.distributions[0]\n",
    "display(distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage III: Data Wrangling:\n",
    "\n",
    "**Purpose:** *Stage III is the section of the Notebook where your Pandas/Python skills come into play, this is where the scraped data (in our case an Excel Spreadsheet) will be transformed from human readable to machine readable format. We call this output transform 'tidy-data'.*\n",
    "\n",
    "**Note:** *For the purposes of this Notebook we're only going to process two tabs of the spreadsheet. Where multiple tabs and multiple outputs are required and generated additional file / code management would obviously be necessary.*\n",
    "\n",
    "**Transforms:** *Our tidy-data outputs are stored in a collection of Pandas DataFrames.*\n",
    "\n",
    "**Validations:** *Validation of the transform will be achieved manually by user; future tools and testing frameworks for COGS are expected to be developed in the near future.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify if execution of code is within a Notebook Environment:\n",
    "boo_pythonNB_env = COGSTransformPack.boo_pythonNB_environment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coding! This is where we code our data transforms. We currently have a requirement within the COGS team to\n",
    "# produce a best practice and hints & tips guide. I personally prefer to data-wrangle using a Pandas dataframe over\n",
    "# using DataBaker (M. Spooner).\n",
    "\n",
    "\n",
    "str_tabsheetsinfocus = 'H' # Only process spreadsheet tabs beginning with this character.\n",
    "if boo_pythonNB_env == True:\n",
    "    COGSTransformPack.printmd(\"**Processing tabs that start with: \" + str_tabsheetsinfocus + \".**\")\n",
    "    \n",
    "i = 0 # Loop Variable.\n",
    "ii = 0 # Loop Variable.\n",
    "dataframe_collection = {} # Collection of Pandas DataFrames.\n",
    "\n",
    "\n",
    "for tab in tabs:\n",
    "    if tabs[i].name.startswith(str_tabsheetsinfocus):\n",
    "        \n",
    "        \n",
    "        if tabs[i].name == 'HE - Country level estimates':\n",
    "            ii = 1\n",
    "            try:\n",
    "                pd_tab = distribution.as_pandas(sheet_name = tabs[i].name) #, skiprows=1, header=None)\n",
    "                pd_df_dimensions = pd_tab.iloc[:, :7] # This split is lost in our transformation - but it helped here.\n",
    "                pd_df_observations = pd_tab.iloc[:, 7:14] # See above.\n",
    "                pd_df_original = pd.concat([pd_df_dimensions, pd_df_observations], axis=1, sort=False)\n",
    "                pd_df_transformed = pd_df_original.dropna(how='all')\n",
    "                pd_df_transformed = pd_df_transformed.drop(columns=['Country', 'sex1', 'ageband'])\n",
    "                pd_df_transformed['Period'] = pd_df_transformed['Period']\\\n",
    "                    .map(lambda x: f'gregorian-interval/{str(x)[:4]}-03-31T00:00:00/P3Y')\n",
    "                pd_df_transformed.loc[pd_df_transformed['Sex'] == 'Males', 'Sex'] = 'M'\n",
    "                pd_df_transformed.loc[pd_df_transformed['Sex'] == 'Females', 'Sex'] = 'F'\n",
    "                pd_df_transformed['age group'][pd_df_transformed['age group'] == '<1'] = 'lessthan1'\n",
    "                pd_df_transformed['age group'][pd_df_transformed['age group'] == '90+'] = '90plus'\n",
    "                pd_df_transformed_le =\\\n",
    "                    pd_df_transformed[['Period', 'Code', 'Sex', 'age group', 'Life Expectancy (LE)_',\\\n",
    "                                       'LE Lower CI_', 'LE Upper CI_',\\\n",
    "                                       #'Proportion of Life Spent in \"Good\" Health (%)_']].copy()\n",
    "                                      ]].copy()\n",
    "                pd_df_transformed_hle =\\\n",
    "                    pd_df_transformed[['Period', 'Code', 'Sex', 'age group', 'Healthy Life Expectancy (HLE) _',\\\n",
    "                                       'HLE Lower CI_', 'HLE Upper CI_',\\\n",
    "                                       #'Proportion of Life Spent in \"Good\" Health (%)_']].copy()\n",
    "                                       ]].copy()\n",
    "                pd_df_transformed_le['TransformationType'] = 'LE'\n",
    "                pd_df_transformed_hle['TransformationType'] = 'HLE'\n",
    "                dataframe_collection[tabs[i].name + '_LE'] = pd_df_transformed_le\n",
    "                dataframe_collection[tabs[i].name + '_HLE'] = pd_df_transformed_hle\n",
    "                COGSTransformPack.printmd('[' + str(i) + '] Processed: ' + tabs[i].name + '.', colour='Green')\n",
    "            except ERR_HECountryLevelEstimates:\n",
    "                print('Error within ' + str(pd_df_name.append(tabs[i].name)) + ' process to extract to Pandas DF.')\n",
    "            \n",
    "            \n",
    "        if tabs[i].name == 'HE - Region level estimates':\n",
    "            ii = 1\n",
    "            try:\n",
    "                pd_tab = distribution.as_pandas(sheet_name = tabs[i].name)\n",
    "                pd_df_dimensions = pd_tab.iloc[:, :8]\n",
    "                pd_df_observations = pd_tab.iloc[:, 8:14]\n",
    "                pd_df_original = pd.concat([pd_df_dimensions, pd_df_observations], axis=1, sort=False)\n",
    "                pd_df_original.columns = pd_df_original.iloc[0]\n",
    "                pd_df_original = pd_df_original[1:]\n",
    "                pd_df_transformed = pd_df_original.dropna(how='all')\n",
    "                pd_df_transformed = pd_df_transformed.drop(columns=['Area_name', 'sex1', 'ageband'])\n",
    "                pd_df_transformed['Period'] = pd_df_transformed['Period']\\\n",
    "                    .map(lambda x: f'gregorian-interval/{str(x)[:4]}-03-31T00:00:00/P3Y')\n",
    "                pd_df_transformed.loc[pd_df_transformed['Sex'] == 'Males', 'Sex'] = 'M'\n",
    "                pd_df_transformed.loc[pd_df_transformed['Sex'] == 'Females', 'Sex'] = 'F'\n",
    "                pd_df_transformed['Age group'][pd_df_transformed['Age group'] == '<1'] = 'lessthan1'\n",
    "                pd_df_transformed['Age group'][pd_df_transformed['Age group'] == '90+'] = '90plus'\n",
    "                pd_df_transformed_le =\\\n",
    "                    pd_df_transformed[['Period', 'Code', 'Sex', 'Age group', 'Life Expectancy (LE)_',\\\n",
    "                                       'LE Lower CI_', 'LE Upper CI_',\\\n",
    "                                       #'Proportion of Life Spent in \"Good\" Health (%)_']].copy()\n",
    "                                       ]].copy()\n",
    "                pd_df_transformed_hle =\\\n",
    "                    pd_df_transformed[['Period', 'Code', 'Sex', 'Age group', 'Healthy Life Expectancy (HLE) _',\\\n",
    "                                       'HLE Lower CI_', 'HLE Upper CI_',\\\n",
    "                                       #'Proportion of Life Spent in \"Good\" Health (%)_']].copy()\n",
    "                                        ]].copy()\n",
    "                pd_df_transformed_le['TransformationType'] = 'LE'\n",
    "                pd_df_transformed_hle['TransformationType'] = 'HLE'\n",
    "                dataframe_collection[tabs[i].name + '_LE'] = pd_df_transformed_le\n",
    "                dataframe_collection[tabs[i].name + '_HLE'] = pd_df_transformed_hle\n",
    "                COGSTransformPack.printmd('[' + str(i) + '] Processed: ' + tabs[i].name + '.', colour='Green')\n",
    "            except ERR_HERegionLevelEstimates:\n",
    "                print('Error within ' + str(pd_df_name.append(tabs[i].name)) + ' process to extract to Pandas DF.')            \n",
    "    \n",
    "        if ii == 0:\n",
    "                COGSTransformPack.printmd('[' + str(i) + '] Ignoring: ' + tabs[i].name + '.', colour='Red')\n",
    "        \n",
    "    i += 1\n",
    "    ii = 0 # Code should be amended to utilise loop break outs...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage IV (a): Data Wrangling Human Inspection:\n",
    "\n",
    "**Purpose:** *This stage is intended so that the developer (you) can 'eye-ball' the 'tidy-data' transform.*\n",
    "\n",
    "**Note:** *This is for human visualisation - not for specific testing (or as part of a testing framework).*\n",
    "\n",
    "**Important:** *If the transformations appear correct we can proceed, otherwise we need to re-factor the data wrangling stage (Stage III).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display your transform. Set the number of rows you'd like to display on screen. At present there's no functionality\n",
    "# for random sampling or switching bewteen the head or tail of the data (possible future iteration(s)).\n",
    "COGSTransformPack.display_DF_collection(dataframe_collection, 'Transform(s) for Inspection:', 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage IV (b): Data Wrangling Validity of our Transforms in CSV  Format:\n",
    "\n",
    "**Purpose:** *This stage is intended so that the developer (you) can 'eye-ball' the 'tidy-data' transform.*\n",
    "\n",
    "**Note:** *This is for human visualisation - not for specific testing (or as part of a testing framework).*\n",
    "\n",
    "**Important:** *If the transformations appear correct we can proceed, otherwise we need to re-factor the data wrangling stage (Stage III).*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the validity of the transform(s) in their CSV format.\n",
    "# display_DF_collection_csv_report will convert the transforms to CSV in memory and perform an analysis.\n",
    "# You will be warned of any validation failures from the analysis.\n",
    "COGSTransformPack.display_DF_collection_csv_report(dataframe_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage IV (c): Isolated Notebook Test(s):\n",
    "\n",
    "**Purpose:** *Until a testing framework is designed and deployed testing of your 'tidy-data' is done manually.*\n",
    "\n",
    "**Description:** *Tests can be programatically implemented using code / functions OR as implemented here via hardcoded values; it's up to you as an engineer until a framework arrives. Until the testing strategy is defined for our COGS development teams, checks that mirror those below are a good starting point; these include counts and sum'ing to ensure no data is lost during the transform process and random data entry point checks to ensure the data wrangling steps have not skewed the data schema / structure.*\n",
    "\n",
    "*A testing framework should be implemented where tests are executed periodically to identify when new source data is available or has been revised - a test suite for a RAP (Reproducible Analytical Pipeline) can highlight failures and a requirment for a code-refactor.*\n",
    "\n",
    "**Important:** *Remember that if testing values are hardcoded they will only apply to a specific data source; for example record counts may change between revisions of the same data source etc.*\n",
    "\n",
    "**FOR INFORMATION! BE ADVISED - Intentionally one test here fails for demonstration purposes.**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay then, it's up to you. Get testing!\n",
    "# Note: Schema test would be hugely beneficial for RAPs.\n",
    "# Hard-coded tests:\n",
    "test_count_int = 0\n",
    "test_count_successful = 0\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "if dataframe_collection['HE - Country level estimates_HLE'].shape[0] == 1600:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "my_testdata = (dataframe_collection['HE - Country level estimates_HLE'].loc[(dataframe_collection['HE - Country level estimates_HLE']['Period'] == 'gregorian-interval/2009-03-31T00:00:00/P3Y') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['Code'] == 'E92000001') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['age group'] == 'lessthan1') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['Sex'] == 'M')]\n",
    "                 )\n",
    "my_expecteddata = pd.DataFrame({\n",
    "                'Period': ['gregorian-interval/2009-03-31T00:00:00/P3Y'],\n",
    "                'Code': ['E92000001'],\n",
    "                'Sex': ['M'],\n",
    "                'age group': ['lessthan1'],\n",
    "                'Healthy Life Expectancy (HLE) _': [63.02647],\n",
    "                'HLE Lower CI_': [62.87787],\n",
    "                'HLE Upper CI_': [63.17508],\n",
    "                #'Proportion of Life Spent in \"Good\" Health (%)_': [80.0024],\n",
    "                'TransformationType': ['HLE']\n",
    "                })\n",
    "if my_testdata.equals(my_expecteddata):\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "my_testdata = (dataframe_collection['HE - Country level estimates_HLE'].loc[(dataframe_collection['HE - Country level estimates_HLE']['Period'] == 'gregorian-interval/2016-03-31T00:00:00/P3Y') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['Code'] == 'W92000004') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['Sex'] == 'F') &\n",
    "                 (dataframe_collection['HE - Country level estimates_HLE']['age group'] == '05-09')]\n",
    "                ) \n",
    "my_testdata.index = np.arange(1,len(my_testdata)+1) # To avoid index comparison errors.\n",
    "my_expecteddata = pd.DataFrame({\n",
    "                'Period': ['gregorian-interval/2016-03-31T00:00:00/P3Y'],\n",
    "                'Code': ['W92000004'],\n",
    "                'Sex': ['F'],\n",
    "                'age group': ['05-09'],\n",
    "                'Healthy Life Expectancy (HLE) _': [57.56803],\n",
    "                'HLE Lower CI_': [57.10483],\n",
    "                'HLE Upper CI_': [58.03124],\n",
    "                #'Proportion of Life Spent in \"Good\" Health (%)_': [74.21282],\n",
    "                'TransformationType': ['HLE']\n",
    "                })\n",
    "my_expecteddata.index = np.arange(1,len(my_expecteddata)+1) # To avoid index comparison errors.\n",
    "if my_testdata.equals(my_expecteddata):\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "if dataframe_collection['HE - Country level estimates_LE']['Life Expectancy (LE)_'].sum() == 65114.214550000004:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')        \n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "if dataframe_collection['HE - Region level estimates_HLE'].shape[0] == 2880:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "if 1 == 2: # Intentional FAILED test for demo purposes...\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')        \n",
    "\n",
    "\n",
    "test_count_int += 1 # Increment Test Counter...\n",
    "if dataframe_collection['HE - Region level estimates_LE']['Life Expectancy (LE)_'].sum() == 118840.88113000023:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Successful.', colour='Green')\n",
    "    test_count_successful += 1 # Increment Successful Test Counter...\n",
    "else:\n",
    "    COGSTransformPack.printmd('Test ID: [' + str(test_count_int) + '] Failed.', colour='Red')\n",
    "                      \n",
    "COGSTransformPack.printmd('**Test Rating: ' + str(round(test_count_successful/test_count_int*100,2)) + '% Successful.**', colour='Magenta')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage IV (?): Isolated Notebook Test(s) - Useful Tip:\n",
    "\n",
    "**Did you know?:** *Having trouble using Python 'assert' or Panda's Dataframe equality checks? A lot of the time, it's the schema that doesn't 'match' even though the data entries appear to be the same.*\n",
    "\n",
    "***Try the following '.info(verbose=True) code-snippet to investigate schemas:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a schema - usually an assert will fail where two schemas do not match i.e. float32 to float64 will\n",
    "# fail even if the data entries match.\n",
    "my_expecteddata.info(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage V (a): Creating & Mapping Reference Data and Data Markers - Identifying the Data Elements:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must isolate the dimensions (keys) from the observations (values) by...*\n",
    "\n",
    "* *Generating the components from the transforms;*\n",
    "* *Strip out data elements - remove all dimensions to leave only the observations thus providing our codelist keys.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of columns / collection of component elements that will be used to define the codelists.\n",
    "codelist_cols = []\n",
    "dataframe_elements_collection = {}\n",
    "\n",
    "codelist_cols = COGSTransformPack.get_component_elements(dataframe_collection)\n",
    "dataframe_elements_collection = COGSTransformPack.get_transform_component_schema(dataframe_collection, codelist_cols)\n",
    "\n",
    "COGSTransformPack.display_DF_collection(dataframe_elements_collection, 'Initial Component DataFrames:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage V (b): Creating & Mapping Reference Data and Data Markers - Assign the Data Elements:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must manually assign the element type:*\n",
    "\n",
    "* *By default the element type is set to 'Dimension';*\n",
    "* *Define the observations in the data item catalogues by...:*\n",
    "* *using the assign_transform_component_type() component in the following code cells manually code the appropriate element types ('Dimension', 'Observation' or 'Measure').*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently does not support assignment of different types where the same element name is present in multiple\n",
    "# catalogue items but you require different types!\n",
    "# Note code here is in-efficient due to multiple passes of the collection in the function per line below:\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'Life Expectancy (LE)_', 'Measure')\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'LE Lower CI_', 'Observation')\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'LE Upper CI_', 'O')\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'Healthy Life Expectancy (HLE) _', 'M')\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'HLE Lower CI_', 'o')\n",
    "COGSTransformPack.assign_transform_component_type(dataframe_elements_collection, 'HLE Upper CI_', 'o')\n",
    "\n",
    "# Display your assignments:\n",
    "str_display_revised = ('Revised Component DataFrames: <br>WARNING!:** Please be AWARE that you should not proceed until the data items are correctly assigned as being either a Dimension or an Observation. <br>If the results below show errors please fix by re-factoring your code and re-run**!')\n",
    "COGSTransformPack.display_DF_collection(dataframe_elements_collection, str_display_revised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage V (c): Creating & Mapping Reference Data and Data Markers - Generating Codelists:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *Function generated code-lists:*\n",
    "\n",
    "* *Codelists are created from the Dimension elements as assigned.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codelist generation via component - get_codelists_from_dimensions():\n",
    "dataframe_codelists_collection = {}\n",
    "dataframe_codelists_collection = COGSTransformPack.get_codelists_from_dimensions(dataframe_collection, dataframe_elements_collection)\n",
    "\n",
    "# Display your Codelists:\n",
    "COGSTransformPack.display_DF_collection(dataframe_codelists_collection, 'Codelists (Generated from get_codelists_from_dimensions()):', 20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage VI (a): Mapping our Transforms with our Reference Data - Reference Data Load:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must map / associate our transformed data dimensions with our reference data repository / master database:*\n",
    "* Load to memory our master reference data.\n",
    "* Map / associate reference data with our transformed data entities / dimensions.\n",
    "    * *Note an automated attempt to map is conducted - but manual intervention is likely.*\n",
    "    * *Mapping between the transformed dimensions (components) and the master reference data is completed manually where the automation routine fails.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the source of the reference data master (currently COGS has a split-by-data-family configuration):\n",
    "url_ref_repo_components = \"https://raw.githubusercontent.com/GSS-Cogs/family-disability/master/reference/components.csv\"\n",
    "url_ref_repo_columns = \"https://raw.githubusercontent.com/GSS-Cogs/family-disability/master/reference/columns.csv\"\n",
    "\n",
    "df_ref_repo_columns = pd.read_csv(url_ref_repo_columns)\n",
    "COGSTransformPack.printmd('**Displaying: ' + url_ref_repo_columns + ':**')\n",
    "display(df_ref_repo_columns)\n",
    "\n",
    "df_ref_repo_components = pd.read_csv(url_ref_repo_components)\n",
    "COGSTransformPack.printmd('**Displaying: ' + url_ref_repo_components + ':**')\n",
    "display(df_ref_repo_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage VI (b): Mapping our Transforms with our Reference Data - Reference / Transform Automated Map:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must map / associate our transformed data dimensions with our reference data repository / master database:*\n",
    "* Load to memory our master reference data.\n",
    "* Map / associate reference data with our transformed data entities / dimensions.\n",
    "    * *Note an automated attempt to map is conducted - but manual intervention is likely.*\n",
    "    * *Mapping between the transformed dimensions (components) and the master reference data is completed manually where the automation routine fails.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated mapping of transformed components with the master reference data:\n",
    "dataframe_mapped_elements_collection = {}\n",
    "dataframe_mapped_elements_collection_errors = {}\n",
    "\n",
    "dataframe_mapped_elements_collection, dataframe_mapped_elements_collection_errors =\\\n",
    "    COGSTransformPack.map_REFData_with_Transform(dataframe_elements_collection,\\\n",
    "                                                 df_ref_repo_columns, df_ref_repo_components)\n",
    "\n",
    "COGSTransformPack.display_DF_collection(dataframe_mapped_elements_collection_errors, 'WARNING - PLEASE ADDRESS NON-MATCHING REFERENCES:** <br>In the proceeding code cells you need to ensure all transform components are matched to a reference item**!')\n",
    "COGSTransformPack.display_DF_collection(dataframe_mapped_elements_collection, 'These are the components that automatically matched:** <br>Please use caution before proceeding - these automated matches may be incorrect**!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage VI (c): Mapping our Transforms with our Reference Data - Reference / Transform Manual Map:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must map / associate our transformed data dimensions with our reference data repository / master database:*\n",
    "* Load to memory our master reference data.\n",
    "* Map / associate reference data with our transformed data entities / dimensions.\n",
    "    * *Note an automated attempt to map is conducted - but manual intervention is likely.*\n",
    "    * *Mapping between the transformed dimensions (components) and the master reference data is completed manually where the automation routine fails.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual mapping of transformed components with the master reference data:\n",
    "# Iterate Code until all components are Mapped:\n",
    "\n",
    "# Take a copy so that this cell can be re-run / iterated until all components are mapped:\n",
    "# This could be factored into the module / function using recursive code; at present the approach is simplified.\n",
    "dataframe_elements_collection_my_manual_edit = {}\n",
    "for key in dataframe_elements_collection.keys(): \n",
    "    dataframe_elements_collection_my_manual_edit[key] = dataframe_elements_collection[key].copy()\n",
    "    \n",
    "# Manual component reference data assignment (where the automated process failed to map or mapped incorrectly):\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'Code', 'ONS Geography')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'age group', 'ONS Age Range')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'Age group', 'ONS Age Range')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'LE Lower CI_', 'Lower CI')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'HLE Lower CI_', 'Lower CI')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'LE Upper CI_', 'Upper CI')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'HLE Upper CI_', 'Upper CI')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'Life Expectancy (LE)_', 'Value')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'Healthy Life Expectancy (HLE) _', 'Value')\n",
    "COGSTransformPack.assign_reference_data_mapping(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                'TransformationType', 'Life Expectancy Estimate Type')\n",
    "\n",
    "dataframe_elements_collection_my_manual_edit = COGSTransformPack.get_mapped_elements(dataframe_elements_collection_my_manual_edit)\n",
    "\n",
    "# Re-run Component:\n",
    "dataframe_mapped_elements_collection, dataframe_mapped_elements_collection_errors =\\\n",
    "    COGSTransformPack.map_REFData_with_Transform(dataframe_elements_collection_my_manual_edit,\\\n",
    "                                                 df_ref_repo_columns, df_ref_repo_components, 'Manual')\n",
    "\n",
    "# Output the results of your manual maps:\n",
    "mapped_message = \"ALL Data Entities Mapped! Looks like you're safe to Proceed!\"\n",
    "if len(dataframe_mapped_elements_collection_errors[key]) > 0:\n",
    "    error_message = 'WARNING - PLEASE ADDRESS NON-MATCHING REFERENCES:** <br>Please continue iterating your manual edits to ensure all transform components are matched to a reference item**!'\n",
    "    mapped_message = 'The following components have successfully mapped:'\n",
    "    COGSTransformPack.display_DF_collection(dataframe_mapped_elements_collection_errors, error_message)\n",
    "COGSTransformPack.display_DF_collection(dataframe_mapped_elements_collection, mapped_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage VI (d): Mapping our Transforms with our Reference Data - Creating the Meta-JSON:\n",
    "\n",
    "**Purpose:** *To create an RDF output we need the data (.csv) and associated meta data (.json).*\n",
    "\n",
    "**Description:** *We must map / associate our transformed data dimensions with our reference data repository / master database:*\n",
    "* Load to memory our master reference data.\n",
    "* Map / associate reference data with our transformed data entities / dimensions.\n",
    "    * *Note an automated attempt to map is conducted - but manual intervention is likely.*\n",
    "    * *Mapping between the transformed dimensions (components) and the master reference data is completed manually where the automation routine fails.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_mapped_elements_collection_names = []\n",
    "for key in dataframe_mapped_elements_collection.keys():\n",
    "    dataframe_mapped_elements_collection_names.append(key)\n",
    "\n",
    "json_meta_data_collection = {}\n",
    "str_distribution_metadata = str(distribution._properties_metadata)\n",
    "json_meta_data_collection = COGSTransformPack.create_meta_json(dataframe_mapped_elements_collection, dataframe_mapped_elements_collection_names, str_distribution_metadata)\n",
    "\n",
    "for key in json_meta_data_collection.keys():\n",
    "    write_to_file_JSONmetadata = open(COGSTransformPack.slug.slug(key) + '-metadata.json', \"w\")\n",
    "    write_to_file_JSONmetadata.write(json_meta_data_collection[key])\n",
    "    write_to_file_JSONmetadata.close()\n",
    "    COGSTransformPack.printmd('**Displaying Constructed meta-JSON for: ' + key + ' :**', colour = 'Green')\n",
    "    print(json_meta_data_collection[key])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage VII: RDF (Resource Description Framework File) Production:\n",
    "\n",
    "**Purpose:** *To create the RDF output(s).*\n",
    "\n",
    "**Description:** *Utilising all the components / objects we have so far constructed we can produce the final RDF(s):*\n",
    "* Until we can process files with the CSVWConverter locally we will utilise GIT for our HTTP requests.\n",
    "* Map / associate reference data with our transformed data entities / dimensions.\n",
    "    * *Push related files to GIT for a single output.*\n",
    "    * *Produce RDF.*\n",
    "    * *---> Iterate until all of our CSV collection is processed <---*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step:\n",
    "COGSTransformPack.create_rdf(dataframe_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to consider:\n",
    "\n",
    "* **Investigate PropertyURL - is this sourced correctly?**\n",
    "\n",
    "* **Investigate AboutURL - is this sourced correctly?**\n",
    "\n",
    "* **What is being done for data markers (eg: NA, Other, See Ref etc.).**\n",
    "\n",
    "* **Testing strategy needed - how do we manage test data between Publishing revisions?**\n",
    "\n",
    "* **Fuzzy matching on codelists to suggest appropriate REFData.**\n",
    "\n",
    "* **Automated matching on component type (dimension, measure or observation) using the REFData?**\n",
    "\n",
    "* **Integration with Leigh's AirTable-GIT API. For example this document could be generated with the transform names already pre-populated. This may eliminate the issue of tracability we currently have of one-source to multiple-outputs where the lineage is lost through 'unexpected' labels and naming of the outputs etc. Aditionally this API could also update AirTable sources to indicate when the Transform process steps are completed etc. (Design, Transform, REFData and RDF etc.).**\n",
    "\n",
    "* **RDF Validation and interfacing with Swirl / PMD Platform.**\n",
    "\n",
    "### Suggestion:\n",
    "\n",
    "* **Sharing the development and learning activities within the team - Shannon and Mike are looking at GSSUtils and Data Blogs / Visualisations etc. so JJ and Vamshi could maybe help develop components of this Notebook (maybe extracting out to APIs for external publishers long term etc)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Pip install fuzzywuzzy\n",
    "!Pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio('spoonbarz', 'spoonbar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzz.ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\")\n",
    "#fuzz.partial_ratio(\"New York Mets vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\")\n",
    "fuzz.token_sort_ratio(\"New York Metz vs Atlanta Braves\", \"Atlanta Braves vs New York Mets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#COGSTransformPack.display_DF_collection(dataframe_codelists_collection, 'Codelists (Generated from get_codelists_from_dimensions()):', 20)\n",
    "import requests\n",
    "import json\n",
    "import urllib.parse\n",
    "import ntpath\n",
    "\n",
    "url = 'http://gss-data.org.uk/sparql.json?query=' + urllib.parse.quote(\"SELECT * WHERE {<http://gss-data.org.uk/def/concept-scheme/ethnic-groups> <http://www.w3.org/2004/02/skos/core#member> ?result}\")\n",
    "print(url)\n",
    "\n",
    "x = requests.post(url)\n",
    "\n",
    "print(x.text)\n",
    "\n",
    "y = json.loads(x.text)\n",
    "\n",
    "#display(y)\n",
    "#display(y['results']['bindings'][0]['result']['value'])\n",
    "#print(ntpath.basename(y['results']['bindings'][0]['result']['value']))\n",
    "\n",
    "#for key in y['results']['bindings']:\n",
    "#    print(key)\n",
    "\n",
    "i = 0\n",
    "str_REFData_In_Scope = ''\n",
    "for key in y['results']['bindings']:\n",
    "    print(y['results']['bindings'][i]['result']['value'])\n",
    "    print(ntpath.basename(y['results']['bindings'][i]['result']['value']))\n",
    "    str_REFData_In_Scope = str_REFData_In_Scope + (ntpath.basename(y['results']['bindings'][i]['result']['value'])) + ', '\n",
    "    i += 1\n",
    "    \n",
    "\n",
    "for key in dataframe_codelists_collection.keys():\n",
    "    for col in dataframe_codelists_collection[key]:\n",
    "        print(col, key)\n",
    "        str_codelist = dataframe_codelists_collection[key][col].dropna().values.tolist()\n",
    "        display(str_codelist)\n",
    "        fuz_matching_rating = fuzz.token_sort_ratio(str_REFData_In_Scope, str_codelist)\n",
    "        print('Fuzzy: ' + str(fuz_matching_rating))\n",
    "\n",
    "# Basic Hacked for 100 Fuzzy Match.\n",
    "str_REFData_In_Scope = 'E92000001, K02000001'       \n",
    "for key in dataframe_codelists_collection.keys():\n",
    "    for col in dataframe_codelists_collection[key]:\n",
    "        print(col, key)\n",
    "        str_codelist = dataframe_codelists_collection[key][col].dropna().values.tolist()\n",
    "        display(str_codelist)\n",
    "        fuz_matching_rating = fuzz.token_sort_ratio(str_REFData_In_Scope, str_codelist)\n",
    "        print('Fuzzy: ' + str(fuz_matching_rating))        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://gss-data.org.uk/sparql.json?query=' + urllib.parse.quote(\"SELECT * WHERE {<http://gss-data.org.uk/def/concept-scheme/phe-sex> <http://www.w3.org/2004/02/skos/core#member> ?result}\")\n",
    "print(url)\n",
    "\n",
    "x = requests.post(url)\n",
    "\n",
    "print(x.text)\n",
    "\n",
    "y = json.loads(x.text)\n",
    "\n",
    "#display(y)\n",
    "#display(y['results']['bindings'][0]['result']['value'])\n",
    "#print(ntpath.basename(y['results']['bindings'][0]['result']['value']))\n",
    "\n",
    "#for key in y['results']['bindings']:\n",
    "#    print(key)\n",
    "\n",
    "i = 0\n",
    "for key in y['results']['bindings']:\n",
    "    print(y['results']['bindings'][i]['result']['value'])\n",
    "    print(ntpath.basename(y['results']['bindings'][i]['result']['value']))\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dataframe_codelists_collection.keys():\n",
    "    for col in dataframe_codelists_collection[key]:\n",
    "        print(col, key)\n",
    "        str_codelist = dataframe_codelists_collection[key][col].dropna().values.tolist()\n",
    "        display(str_codelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyDictionary\n",
    "from PyDictionary import PyDictionary\n",
    "\n",
    "dictionary=PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (dictionary.synonym(\"gender\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "#Creating a list \n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"employment\"):\n",
    "    for lm in syn.lemmas():\n",
    "             synonyms.append(lm.name())#adding into synonyms\n",
    "print (set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gssutils latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
